{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Founders and Financials: Machine Learning Algorithms in Venture Capital by Viktor Lado Naess and Emrik St√•l\n",
        "\n"
      ],
      "metadata": {
        "id": "kTjCz3xbKg_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9kWV_hqmlWB"
      },
      "outputs": [],
      "source": [
        "## getting system info\n",
        "\n",
        "import sys\n",
        "\n",
        "print(\"Python version\")\n",
        "print(sys.version)\n",
        "print(\"Version info.\")\n",
        "print(sys.version_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQPrvnn0uxUv"
      },
      "source": [
        "#START\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO9_RRBh7W1Q"
      },
      "outputs": [],
      "source": [
        "# import our google sheet that was the data from the end of stata\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "spreadsheet = gc.open('Unclean_data')\n",
        "\n",
        "worksheet = spreadsheet.worksheet('Sheet1')\n",
        "\n",
        "\n",
        "data = worksheet.get_all_values()\n",
        "\n",
        "import pandas as pd\n",
        "Original_df = pd.DataFrame(data[1:], columns=data[0])\n",
        "Original_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuP1NG3eddZ4"
      },
      "outputs": [],
      "source": [
        "#renmaming columns to more intuitive names\n",
        "Original_df.rename(columns={'rr07_rorreul': 'Operating_profit'}, inplace=True)\n",
        "Original_df.rename(columns={'rr12_resefin': 'Profit_after_net_Financial_Income/expenses'}, inplace=True)\n",
        "Original_df.rename(columns={'rr15_resar': 'Net_profit_for_year'}, inplace=True)\n",
        "Original_df.rename(columns={'ser_nystartat': 'Founed_this_year'}, inplace=True)\n",
        "Original_df.rename(columns={'rr01_ntoms': 'Net_sales'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_kapomsh': 'Asset_turnover_ratio'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_avtokap': 'Return_on_assets'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_solid': 'Equity/asset_ratio'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_avkegkap': 'Return_on_equity'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_kassalikv': 'Quick_ratio'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_rormarg': 'Operating_margin'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_nettomarg': 'Net_margin'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_vinstprc': 'Profit_margin'}, inplace=True)\n",
        "Original_df.rename(columns={'ny_omsf': 'Change_Net_sales'}, inplace=True)\n",
        "Original_df.rename(columns={'bransch_borsbransch_konv': 'Branch_sector'}, inplace=True)\n",
        "Original_df.rename(columns={'ser_regdat': 'Reg_Date'}, inplace=True)\n",
        "Original_df.rename(columns={'regYear ': 'Reg_year'}, inplace=True)\n",
        "Original_df.columns = [col.strip() for col in Original_df.columns]\n",
        "import numpy as np\n",
        "#replacing empty values with these columns with 0\n",
        "columns_to_replace_with_zero = ['IPO', 'Merger_Acqusition', 'Serial_Founder']\n",
        "for column in columns_to_replace_with_zero:\n",
        "    Original_df[column].replace('', np.nan, inplace=True)\n",
        "    Original_df[column].replace(np.nan, 0, inplace=True)\n",
        "    Original_df[column] = Original_df[column].astype(int)\n",
        "#replacing empty sections with namn\n",
        "for column in Original_df.columns:\n",
        "    Original_df[column].replace('', np.nan, inplace=True)\n",
        "#chaning Name column to a string\n",
        "Original_df.loc[:,'Name'] = Original_df['Name'].astype(str)\n",
        "Original_df['Name'] = Original_df['Name'].str.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following section we remove any rows that have too little data to be useful, intially we remove rows with non of the three financial variables but ultimately we remove all rows that are missing one of the three financial varibales as after some investigaiton there are few rows to gain if one were to implement kmeans. The investigation can be done by uncommenting diffent lines of code in the second chunk after this section."
      ],
      "metadata": {
        "id": "is7iufDuweRB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJHyCU5hCGsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "triple_na_rows = Original_df[Original_df['Quick_ratio'].isna() & Original_df['Return_on_equity'].isna() & Original_df['Net_sales'].isna()]\n",
        "\n",
        "\n",
        "Original_df = Original_df.drop(triple_na_rows.index)\n",
        "Original_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNwSYlYKJmzV"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Quick_ratio X Net_sales\n",
        "# na_rows = Original_df[Original_df['Quick_ratio'].isna() & Original_df['Net_sales'].isna()]\n",
        "\n",
        "#Return_on_equity X Net_sales\n",
        "# na_rows = Original_df[Original_df['Net_sales'].isna() & Original_df['Return_on_equity'].isna()]\n",
        "\n",
        "# na_rows = Original_df[Original_df['Quick_ratio'].isna()]\n",
        "# big_3_na_rows = Original_df[Original_df['Quick_ratio'].isna() | Original_df['Return_on_equity'].isna() | Original_df['Net_sales'].isna() | Original_df['Branch_sector'].isna()]\n",
        "\n",
        "big_3_na_rows = Original_df[Original_df['Quick_ratio'].isna() | Original_df['Return_on_equity'].isna() | Original_df['Net_sales'].isna()]\n",
        "# big_3_na_rows\n",
        "Original_df = Original_df.drop(big_3_na_rows.index)\n",
        "Original_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_QDC3TOq_NP"
      },
      "outputs": [],
      "source": [
        "feature_columns = ['Quick_ratio', 'Return_on_equity', 'Net_sales']\n",
        "for column in feature_columns:\n",
        "  Original_df[column] = Original_df[column].replace('', np.nan)\n",
        "  Original_df = Original_df.dropna(subset=[column])\n",
        "  Original_df.loc[:,column] = Original_df[column].astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f34L0JDpY4-"
      },
      "outputs": [],
      "source": [
        "Original_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2nILAHj71oh"
      },
      "outputs": [],
      "source": [
        "Original_df.dtypes #checking datatypes of columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt0DluCvu89p"
      },
      "source": [
        "#DATA MAPPING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMAF9Eeq8NLX"
      },
      "outputs": [],
      "source": [
        "from os import name\n",
        "\n",
        "#columns are made the appropirate data type\n",
        "df= Original_df.copy()\n",
        "df = df.drop(['ser_year', 'ser_namn','Search_Year'], axis=1)\n",
        "df.loc[:,'Name'] = df['Name'].astype(str)\n",
        "df.loc[:,'Location'] = df['Location'].astype(str)\n",
        "df.loc[:,'Founded_on_Year'] = df['Founded_on_Year'].astype(int)\n",
        "df.loc[:,'More_than_one_round'] = df['More_than_one_round'].astype(int)\n",
        "df.loc[:,'Number_of_rounds'] = df['Number_of_rounds'].astype(int)\n",
        "df.loc[:,'Merger_Acqusition'] = df['Merger_Acqusition'].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckHTHHZPwp3W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #In the following, nan values in IPO and Merger_Acqusition are made 0. This is because we left when making these columns we empty cells represent no IPO Merger/acquisition respectively\n",
        "import numpy as np\n",
        "int_columns = ['IPO', 'Merger_Acqusition']\n",
        "for column in int_columns:\n",
        "  df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "  df[column] = df[column].replace('', np.nan)\n",
        "  df[column] = df[column].fillna(0)\n",
        "  df[column] = df[column].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L-ns87yv23m"
      },
      "outputs": [],
      "source": [
        "#here we drop rows with no branch sector or final second round year\n",
        "int_columns = ['Branch_sector', 'Final_Second_Round_Year']\n",
        "for column in int_columns:\n",
        "  df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "  df[column] = df[column].replace('', np.nan)\n",
        "  df = df.dropna(subset=['Branch_sector'])\n",
        "  df[column] = df[column].astype(int)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgmLfRr_wcDX"
      },
      "outputs": [],
      "source": [
        "#here we drop rows with no Quick_ratio,Return_on_equity, Net_sales but more importantly we make these columns float variables\n",
        "feature_columns = ['Quick_ratio', 'Return_on_equity', 'Net_sales']\n",
        "for column in feature_columns:\n",
        "  df[column] = df[column].replace('', np.nan)\n",
        "  df = df.dropna(subset=[column])\n",
        "  df.loc[:,column] = df[column].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw3-gaz7Cfut"
      },
      "outputs": [],
      "source": [
        "cols = ['Name','More_than_one_round', 'Founded_on_Year', 'Final_Second_Round_Year', 'Distance_from_Stockholm', 'Serial_Founder', 'Quick_ratio', 'Return_on_equity', 'Net_sales', 'Branch_sector', 'Share female founders', 'Location']\n",
        "df=df[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--WL1NN-BjEf"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR3dBpbRw0iy"
      },
      "outputs": [],
      "source": [
        "df.loc[:,'Return_on_equity'] = df['Return_on_equity'].astype(float)\n",
        "df.loc[:,'Net_sales'] = df['Net_sales'].astype(float)\n",
        "df.loc[:,'Quick_ratio'] = df['Quick_ratio'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUMD8HQs9CCJ"
      },
      "outputs": [],
      "source": [
        "df.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmY5_sVAFHxt"
      },
      "outputs": [],
      "source": [
        "df['Location'] = df['Location'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiUx5EFnRA7x"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_cJ6aZErMBT"
      },
      "source": [
        "#adding benchmarked net sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWG4yIz39upG"
      },
      "outputs": [],
      "source": [
        "#Here we implement the net sales benchmark, by importing sheet2 form Unclean_data work sheet\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "spreadsheet = gc.open('Unclean_data')\n",
        "\n",
        "worksheet = spreadsheet.worksheet('Sheet2')\n",
        "#worksheet = spreadsheet.get_worksheet(0)\n",
        "\n",
        "# List all worksheets in the spreadsheet\n",
        "# all_worksheets = spreadsheet.worksheets()\n",
        "# print([ws.title for ws in all_worksheets])\n",
        "\n",
        "\n",
        "data = worksheet.get_all_values()\n",
        "\n",
        "import pandas as pd\n",
        "NS_BM = pd.DataFrame(data[1:], columns=data[0])\n",
        "NS_BM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "34F0h-sVUpAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mg_etDaO4cL"
      },
      "outputs": [],
      "source": [
        "# making sure there is name consistency and data type consistency prior to merger\n",
        "NS_BM.rename(columns={'Year': 'Final_Second_Round_Year'}, inplace=True)\n",
        "NS_BM.rename(columns={'Branch': 'Branch_sector'}, inplace=True)\n",
        "NS_BM.rename(columns={'regYear': 'Founded_on_Year'}, inplace=True)\n",
        "NS_BM.loc[:,'Founded_on_Year'] = NS_BM['Founded_on_Year'].astype(int)\n",
        "\n",
        "int_columns = ['Branch_sector', 'Final_Second_Round_Year']\n",
        "for column in int_columns:\n",
        "  NS_BM[column] = pd.to_numeric(NS_BM[column], errors='coerce')\n",
        "  NS_BM[column] = NS_BM[column].replace('', np.nan)\n",
        "\n",
        "  NS_BM = NS_BM.dropna(subset=['Branch_sector'])\n",
        "  NS_BM[column] = NS_BM[column].astype(int)\n",
        "\n",
        "NS_BM.loc[:,'avg'] = NS_BM['avg'].astype(float)\n",
        "\n",
        "\n",
        "NS_BM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "QWmILY10UDAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the merge with an outer join\n",
        "merged_df = pd.merge(df, NS_BM[['Founded_on_Year', 'Final_Second_Round_Year', 'Branch_sector', 'avg']],\n",
        "                     on=['Founded_on_Year', 'Final_Second_Round_Year', 'Branch_sector'],\n",
        "                     how='outer', indicator=True)\n",
        "\n",
        "# Filter to find rows that were in the original data frame but not in the new import one\n",
        "dropped_rows = merged_df[merged_df['_merge'] == 'left_only']\n",
        "\n",
        "\n",
        "dropped_rows\n"
      ],
      "metadata": {
        "id": "zp1XwwtZVUCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9ukW_jOtwu-"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(df, NS_BM[['Founded_on_Year', 'Final_Second_Round_Year', 'Branch_sector', 'avg']],\n",
        "                     on=['Founded_on_Year', 'Final_Second_Round_Year', 'Branch_sector'],\n",
        "                     how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "3ASFD3bhVDCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlVGPRTlviwQ"
      },
      "outputs": [],
      "source": [
        "#making net sales ratio\n",
        "df['net_sale_ratio'] = (df['Net_sales'] / df['avg'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making sure that the two columns are numeric\n",
        "df['Distance_from_Stockholm'] = pd.to_numeric(df['Distance_from_Stockholm'], errors='coerce')\n",
        "df['Share female founders'] = pd.to_numeric(df['Share female founders'], errors='coerce')"
      ],
      "metadata": {
        "id": "NUTnaLm_pAGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ1YqKYsCgaK"
      },
      "outputs": [],
      "source": [
        "#some finally renaming as to make graphs nicer\n",
        "Namecols = ['Name','More_than_one_round','Share female founders', 'Distance_from_Stockholm', 'Location', 'Serial_Founder', 'Quick_ratio', 'Return_on_equity', 'Net_sales', 'net_sale_ratio']\n",
        "FinNames=df[Namecols]\n",
        "cols = ['More_than_one_round','Share female founders', 'Distance_from_Stockholm', 'Location', 'Serial_Founder', 'Quick_ratio', 'Return_on_equity', 'Net_sales', 'net_sale_ratio' ,'Final_Second_Round_Year', 'Founded_on_Year', 'Branch_sector','avg']\n",
        "\n",
        "df=df[cols]\n",
        "\n",
        "\n",
        "\n",
        "df.rename(columns={'More_than_one_round': 'More than one round'}, inplace=True)\n",
        "df.rename(columns={'Distance_from_Stockholm': 'Distance from Stockholm'}, inplace=True)\n",
        "df.rename(columns={'Serial_Founder': 'Serial Founder'}, inplace=True)\n",
        "df.rename(columns={'Quick_ratio': 'Quick Ratio'}, inplace=True)\n",
        "df.rename(columns={'Return_on_equity': 'Return on Equity'}, inplace=True)\n",
        "df.rename(columns={'net_sale_ratio': 'Net Sales Ratio'}, inplace=True)\n",
        "df.rename(columns={'Branch_sector': 'Branch Sector'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_rows = None\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.expand_frame_repr = False\n",
        "pd.options.display.max_colwidth = -1\n",
        "df"
      ],
      "metadata": {
        "id": "ULBxM0I-m95R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # The below code will make a google sheet with the above dataframe this the final data, that is used in the following sections. However as noted in the README the accuarcy of the data here is rounded to a higher extent. So if this is run\n",
        "## be sure to edit the sheet, specifically set cell i2= h2/m2 and then drag down until row 219.\n",
        "\n",
        "# df.replace([np.nan, np.inf, -np.inf], None, inplace=True)\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "# import gspread\n",
        "# from google.auth import default\n",
        "# creds, _ = default()\n",
        "# gc = gspread.authorize(creds)\n",
        "\n",
        "# sh = gc.create('final data')\n",
        "# worksheet = sh.get_worksheet(0)\n",
        "# worksheet.clear()\n",
        "# worksheet.append_row(df.columns.tolist())\n",
        "# for index, row in df.iterrows():\n",
        "#     worksheet.append_row(row.tolist())\n",
        "\n",
        "# headers = df.columns.tolist()"
      ],
      "metadata": {
        "id": "jULOAHv1nthU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}