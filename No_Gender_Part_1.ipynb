{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Founders and Financials: Machine Learning Algorithms in Venture Capital by Viktor Lado Naess and Emrik Stål\n",
        "\n",
        "Viktor Lado Naess: viktor.ladonaess@gmail.com;  +46706719459\n",
        "\n",
        "Emrik Stål: emrik02@gmail.com; +46735931454"
      ],
      "metadata": {
        "id": "27pyADEcKjhj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IkPGoSKNow8"
      },
      "outputs": [],
      "source": [
        "#importing data from sheet\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "spreadsheet = gc.open('final data')\n",
        "\n",
        "worksheet = spreadsheet.worksheet('Sheet1')\n",
        "\n",
        "\n",
        "data = worksheet.get_all_values()\n",
        "\n",
        "#Assiging correct data types\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data[1:], columns=data[0])\n",
        "import numpy as np\n",
        "df.loc[:,'More than one round'] = df['More than one round'].astype(int)\n",
        "df['Share female founders'] = df['Share female founders'].replace('', np.nan) #We do not intend to filter this so assigning nan instead of ''\n",
        "df.loc[:,'Share female founders'] = df['Share female founders'].astype(float)\n",
        "df.loc[:,'Distance from Stockholm'] = df['Distance from Stockholm'].astype(float)\n",
        "df.loc[:,'Serial Founder'] = df['Serial Founder'].astype(int)\n",
        "df.loc[:,'Branch Sector'] = df['Branch Sector'].astype(int)\n",
        "df.loc[:,'Quick Ratio'] = df['Quick Ratio'].astype(float)\n",
        "df.loc[:,'Return on Equity'] = df['Return on Equity'].astype(float)\n",
        "df.loc[:,'Net_sales'] = df['Net_sales'].astype(float)\n",
        "df.loc[:,'Net Sales Ratio'] = df['Net Sales Ratio'].astype(float)\n",
        "rf_cols = ['More than one round', 'Distance from Stockholm', 'Serial Founder', 'Quick Ratio', 'Return on Equity', 'Net Sales Ratio','Share female founders']\n",
        "df=df[rf_cols]\n",
        "\n",
        "random_seed =8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing outliers"
      ],
      "metadata": {
        "id": "K-BpyE2ows_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# List of columns to iterate through\n",
        "columns = ['Return on Equity', 'Quick Ratio', 'Net Sales Ratio']\n",
        "\n",
        "\n",
        "outlier_indices = set()\n",
        "\n",
        "for col in columns:\n",
        "    # Calculate Z-scores for the column\n",
        "    z_scores = stats.zscore(df[col])\n",
        "\n",
        "    # Find indices of outliers (where Z-score absolute value is greater than 3)\n",
        "    outliers = df[abs(z_scores) > 3].index\n",
        "\n",
        "    # Add outlier indices to the set\n",
        "    outlier_indices.update(outliers)\n",
        "\n",
        "# Drop the outlier rows from the DataFrame\n",
        "df = df.drop(index=outlier_indices)\n",
        "\n",
        "# df_cleaned is now the DataFrame without the outliers\n",
        "df"
      ],
      "metadata": {
        "id": "wzhzj1hGVfgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['More than one round'].value_counts()"
      ],
      "metadata": {
        "id": "vN0YOaTIsvJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random forest"
      ],
      "metadata": {
        "id": "gScGCmITRwUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following section of code is highly intensive for hardware. If decided to run on laptop, make sure all other programmes and tabs are closed and be prepared to do manual restart for google(if on colab) or other running software.\n",
        "\n",
        "\n",
        "It is understood that not all who run this may have the specifications to complete the run (at its fastest it will be 1.5 hours, but commonly between 2-5 hours). As such an simpler version is provided below, which is not intensive. This version  is slightly less accurate however resutls only differ minorly."
      ],
      "metadata": {
        "id": "RkzcWq1ejlL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install optuna\n",
        "# import pandas as pd\n",
        "# import optuna\n",
        "# import numpy as np\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, roc_curve, roc_auc_score\n",
        "# from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "# from sklearn.utils import resample\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# y = df['More than one round']\n",
        "# X = df.drop(columns=['More than one round', 'Share female founders'])\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed, stratify=y)\n",
        "\n",
        "# parameter_ranges = [\n",
        "#     {'n_estimators_range': (50, 200), 'max_depth_range': (1, 5)},\n",
        "#     {'n_estimators_range': (50, 200), 'max_depth_range': (5, 10)},\n",
        "#     {'n_estimators_range': (50, 200), 'max_depth_range': (10, 15)},\n",
        "#     {'n_estimators_range': (200, 550), 'max_depth_range': (1, 5)},\n",
        "#     {'n_estimators_range': (200, 550), 'max_depth_range': (5, 10)},\n",
        "#     {'n_estimators_range': (200, 550), 'max_depth_range': (10, 15)},\n",
        "# ]\n",
        "\n",
        "# results1 = []\n",
        "# random_seed = 8\n",
        "# for param_range in parameter_ranges:\n",
        "#     def objective(trial):\n",
        "\n",
        "#         n_estimators = trial.suggest_int('n_estimators', param_range['n_estimators_range'][0], param_range['n_estimators_range'][1])\n",
        "#         max_depth = trial.suggest_int('max_depth', param_range['max_depth_range'][0], param_range['max_depth_range'][1])\n",
        "#         min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "#         min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
        "#         max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
        "#         # strat_k_fold =trial.suggest_categorical('strat_k_fold', [7,StratifiedKFold(n_splits=7)])\n",
        "\n",
        "#         rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
        "#                                     min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "#                                     max_features=max_features, random_state=random_seed)\n",
        "\n",
        "\n",
        "#         rf.fit(X_train, y_train)\n",
        "#         y_pred_best = rf.predict(X_train)\n",
        "#         accuracy_best = accuracy_score(y_train, y_pred_best)\n",
        "\n",
        "\n",
        "#         #Cross validation training accuracy\n",
        "#         strat_k_fold=StratifiedKFold(n_splits=7, shuffle=True, random_state=random_seed)\n",
        "#         crossAcc = cross_val_score(rf, X_train, y_train, cv=strat_k_fold, n_jobs=-1).mean()\n",
        "\n",
        "\n",
        "#         return accuracy_best, crossAcc\n",
        "\n",
        "\n",
        "#     sampler = optuna.samplers.TPESampler(seed=random_seed)\n",
        "#     study = optuna.create_study(directions=['maximize', 'maximize'], sampler=sampler)\n",
        "#     study.optimize(objective, n_trials=300)\n",
        "\n",
        "#     for trial in study.best_trials:\n",
        "\n",
        "#             best_rf = RandomForestClassifier(**trial.params)\n",
        "#             best_rf.fit(X_train, y_train)\n",
        "\n",
        "#             # Additional metrics calculations\n",
        "#             cv_splitter = KFold(n_splits=7, shuffle=True, random_state=random_seed)\n",
        "\n",
        "\n",
        "#             y_pred_best = best_rf.predict(X_test)\n",
        "#             accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "#             precision_best = precision_score(y_test, y_pred_best)\n",
        "#             recall_best = recall_score(y_test, y_pred_best)\n",
        "#             f1_best = f1_score(y_test, y_pred_best)\n",
        "#             cross_val_best_rf = np.mean(cross_val_score(best_rf, X_train, y_train, cv=cv_splitter))\n",
        "#             cross_val_best_rf_test = np.mean(cross_val_score(best_rf, X_test, y_test, cv=cv_splitter))\n",
        "#             cross_val_best_rf_all = np.mean(cross_val_score(best_rf, X, y, cv=cv_splitter))\n",
        "#             y_pred_probs_best = best_rf.predict_proba(X_test)[:, 1]\n",
        "#             brier_score_best = brier_score_loss(y_test, y_pred_probs_best)\n",
        "#             roc_auc_best = roc_auc_score(y_test, y_pred_probs_best)\n",
        "\n",
        "#             # Bootstrap scores\n",
        "#             rf_bootstrap_scores = []\n",
        "#             n_bootstrap = 100\n",
        "#             for seed in range(8, 8 + n_bootstrap):\n",
        "#                 np.random.seed(seed)\n",
        "#                 X_test_sample, y_test_sample = resample(X_test, y_test)\n",
        "#                 y_pred_sample = best_rf.predict(X_test_sample)\n",
        "#                 score = accuracy_score(y_test_sample, y_pred_sample)\n",
        "#                 rf_bootstrap_scores.append(score)\n",
        "#             rf_average_score = np.mean(rf_bootstrap_scores)\n",
        "#             rf_variance_score = np.var(rf_bootstrap_scores)\n",
        "\n",
        "#             #placebo\n",
        "#             np.random.seed(random_seed)\n",
        "#             X_train_random = np.random.random(size=X_train.shape)\n",
        "#             rf_random = RandomForestClassifier(**trial.params)\n",
        "#             rf_random.fit(X_train_random, y_train)\n",
        "#             y_pred_random = rf_random.predict(X_test)\n",
        "#             accuracy_random = accuracy_score(y_test, y_pred_random)\n",
        "\n",
        "#             # Append results to the list\n",
        "#             results1.append({\n",
        "#                 'Parameter Range': param_range,\n",
        "#                 'Best Hyperparameters': trial.params,\n",
        "#                 'Accuracy': accuracy_best,\n",
        "#                 'Cross Validation test Accuracy':cross_val_best_rf_test,\n",
        "#                 'Bootstrap Average Score': rf_average_score,\n",
        "#                 'Placebo Accuracy': accuracy_random,\n",
        "#                 'Cross Validation train Accuracy': cross_val_best_rf,\n",
        "#                 'Precision': precision_best,\n",
        "#                 'Recall': recall_best,\n",
        "#                 'F1 Score': f1_best,\n",
        "#                 'Brier Score': brier_score_best,\n",
        "#                 'ROC AUC Score': roc_auc_best,\n",
        "#                 'Bootstrap Variance Score': rf_variance_score\n",
        "#             })\n",
        "\n",
        "# # Convert results list to DataFrame\n",
        "# results_df1 = pd.DataFrame(results1)\n",
        "# results_df1\n"
      ],
      "metadata": {
        "id": "YXtm9_jzVxeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results_df1 = pd.DataFrame(results1)\n",
        "# results_df1\n"
      ],
      "metadata": {
        "id": "f60iINEUtHGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results_df1['Sum'] = results_df1['Accuracy'] + results_df1['Cross Validation test Accuracy'] + results_df1['Bootstrap Average Score'] -results_df1['Placebo Accuracy']\n",
        "# results_df1['Processed_Min'] = results_df1[['Accuracy', 'Cross Validation test Accuracy', 'Bootstrap Average Score']].min(axis=1) - results_df1['Placebo Accuracy']\n",
        "\n",
        "# normalized_df = (results_df1 - results_df1.min()) / (results_df1.max() - results_df1.min())\n",
        "# results_df1['sum norm'] = normalized_df['Sum']\n",
        "# results_df1['process norm'] = normalized_df['Processed_Min']\n",
        "\n",
        "# results_df1['final vals'] = results_df1['sum norm'] + results_df1['process norm']\n",
        "# pd.set_option('display.max_rows', None)  # Show all rows\n",
        "# pd.set_option('display.max_columns', None)  # Show all columns\n",
        "# pd.set_option('display.width', None)\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "# results_df1"
      ],
      "metadata": {
        "id": "7a7fDUYB_gll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVscl2tO0ZQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_final_vals_index = results_df1['final vals'].idxmax()\n",
        "\n",
        "# # Extract the row with the highest 'final vals'\n",
        "# highest_val = results_df1.loc[max_final_vals_index]\n",
        "# # highest_val"
      ],
      "metadata": {
        "id": "WPIjNlyFWt6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#rf analysis"
      ],
      "metadata": {
        "id": "u_2AWLy0HRFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# best_hyperparams = highest_val['Best Hyperparameters']\n",
        "# best_rf = RandomForestClassifier(**best_hyperparams)\n",
        "\n",
        "#Use the below code if the Optimization takes too long. Use the above code if the optimizaiton code is run. Be sure to uncomment the non used code in either case\n",
        "\n",
        "best_rf= RandomForestClassifier(max_depth=2, max_features='auto', min_samples_leaf=3, min_samples_split=18, n_estimators=94)\n",
        "y = df['More than one round']\n",
        "X = df.drop(columns=['More than one round', 'Share female founders'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed, stratify=y)\n",
        "best_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LhXRXZKdYEZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "y_pred = best_rf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "disp.ax_.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ARKX2vclhk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature importance weights\n",
        "feature_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "feature_importances"
      ],
      "metadata": {
        "id": "4HkA11TVcMyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bar plot of importances\n",
        "feature_importances.plot.bar();"
      ],
      "metadata": {
        "id": "q0qD2G5ucOdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating partial depeendence plots\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(best_rf, X_train, features=['Distance from Stockholm'], kind = \"average\")\n",
        "PartialDependenceDisplay.from_estimator(best_rf, X_train, features=['Serial Founder'], kind = \"average\")\n",
        "PartialDependenceDisplay.from_estimator(best_rf, X_train, features=['Quick Ratio'], kind = \"average\")\n",
        "PartialDependenceDisplay.from_estimator(best_rf, X_train, features=['Return on Equity'], kind = \"average\")\n",
        "PartialDependenceDisplay.from_estimator(best_rf, X_train, features=['Net Sales Ratio'], kind = \"average\")\n"
      ],
      "metadata": {
        "id": "GZ3oxeRzcP7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing shap package and producing shap summary plots\n",
        "!pip install shap\n",
        "import shap\n",
        "explainer = shap.TreeExplainer(best_rf)\n",
        "shap_values = explainer.shap_values(X_train)\n",
        "shap.summary_plot(shap_values, X_train)"
      ],
      "metadata": {
        "id": "RVo9zcQEccla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scatter Shap summery plot\n",
        "shap.summary_plot(shap_values[1], X_train)"
      ],
      "metadata": {
        "id": "9Xih5Qux05f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shap decision plot- ultimately not used in paper.\n",
        "shap.decision_plot(explainer.expected_value[1], shap_values[1], X_test.columns)"
      ],
      "metadata": {
        "id": "1HpNE7m109W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Partial depende plots for SHAP Values\n",
        "for i in range(len(shap_values)):\n",
        "\n",
        "    for feature_index in range(X_train.shape[1]):\n",
        "        shap.dependence_plot(feature_index, shap_values[1], X_train, interaction_index=None)"
      ],
      "metadata": {
        "id": "4TDWhW3z09OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GX_qYQL_08IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zRzAzBa08wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#compare w logit"
      ],
      "metadata": {
        "id": "PigMenJdcvER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#builindg a logistic model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split, StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "logres = []\n",
        "# Create a Logistic Regression model\n",
        "best_logit = LogisticRegression(max_iter=1000)\n",
        "best_logit.fit(X_train, y_train)\n",
        "\n",
        "# Additional metrics calculations\n",
        "cv_splitter = KFold(n_splits=7, shuffle=True, random_state=random_seed)\n",
        "\n",
        "y_pred_best = best_logit.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "precision_best = precision_score(y_test, y_pred_best)\n",
        "recall_best = recall_score(y_test, y_pred_best)\n",
        "f1_best = f1_score(y_test, y_pred_best)\n",
        "cross_val_best_logit = np.mean(cross_val_score(best_logit, X_train, y_train, cv=cv_splitter))\n",
        "cross_val_best_logit_test = np.mean(cross_val_score(best_logit, X_test, y_test, cv=cv_splitter))\n",
        "cross_val_best_logit_all = np.mean(cross_val_score(best_logit, X, y, cv=cv_splitter))\n",
        "y_pred_probs_best = best_logit.predict_proba(X_test)[:, 1]\n",
        "brier_score_best = brier_score_loss(y_test, y_pred_probs_best)\n",
        "roc_auc_best = roc_auc_score(y_test, y_pred_probs_best)\n",
        "\n",
        "# Bootstrap scores\n",
        "logit_bootstrap_scores = []\n",
        "n_bootstrap = 100\n",
        "for seed in range(8, 8 + n_bootstrap):\n",
        "    np.random.seed(seed)\n",
        "    X_test_sample, y_test_sample = resample(X_test, y_test)\n",
        "    y_pred_sample = best_logit.predict(X_test_sample)\n",
        "    score = accuracy_score(y_test_sample, y_pred_sample)\n",
        "    logit_bootstrap_scores.append(score)\n",
        "logit_average_score = np.mean(logit_bootstrap_scores)\n",
        "logit_variance_score = np.var(logit_bootstrap_scores)\n",
        "\n",
        "# Placebo test\n",
        "np.random.seed(random_seed)\n",
        "X_train_random = np.random.random(size=X_train.shape)\n",
        "logit_random = LogisticRegression()\n",
        "logit_random.fit(X_train_random, y_train)\n",
        "y_pred_random = logit_random.predict(X_test)\n",
        "accuracy_random = accuracy_score(y_test, y_pred_random)\n",
        "\n",
        "logres.append({\n",
        "    'Model': 'Logit',\n",
        "    'Accuracy': accuracy_best,\n",
        "    'Cross Validation test Accuracy': cross_val_best_logit_test,\n",
        "    'Bootstrap Average Score': logit_average_score,\n",
        "    'Placebo Accuracy': accuracy_random,\n",
        "    'Cross Validation train Accuracy': cross_val_best_logit,\n",
        "    'Precision': precision_best,\n",
        "    'Recall': recall_best,\n",
        "    'F1 Score': f1_best,\n",
        "    'Brier Score': brier_score_best,\n",
        "    'ROC AUC Score': roc_auc_best,\n",
        "    'Bootstrap Variance Score': logit_variance_score\n",
        "})\n",
        "\n",
        "# Convert logres list to DataFrame\n",
        "logres_df = pd.DataFrame(logres)\n",
        "logres_df\n"
      ],
      "metadata": {
        "id": "mwT89c7vbHz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns_to_drop = ['Parameter Range', 'Best Hyperparameters', 'Sum', 'Processed_Min', 'sum norm', 'process norm','final vals']\n",
        "\n",
        "# # Drop the specified columns\n",
        "# highest_val['Model'] = 'Random forest'\n",
        "# clean_highest_val = highest_val.drop(columns_to_drop)\n",
        "# logres_df = logres_df.append(clean_highest_val, ignore_index=True)\n",
        "\n",
        "\n",
        "#The below code can be used if The optimization code is not run or does not complete its run. If the optimization code is run be sure to sure uncomment the above code and comment the below code.\n",
        "\n",
        "best_rf.fit(X_train, y_train)\n",
        "\n",
        "# Additional metrics calculations\n",
        "cv_splitter = KFold(n_splits=7, shuffle=True, random_state=random_seed)\n",
        "\n",
        "y_pred_best = best_rf.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "precision_best = precision_score(y_test, y_pred_best)\n",
        "recall_best = recall_score(y_test, y_pred_best)\n",
        "f1_best = f1_score(y_test, y_pred_best)\n",
        "cross_val_best_rf = np.mean(cross_val_score(best_rf, X_train, y_train, cv=cv_splitter))\n",
        "cross_val_best_rf_test = np.mean(cross_val_score(best_rf, X_test, y_test, cv=cv_splitter))\n",
        "cross_val_best_rf_all = np.mean(cross_val_score(best_rf, X, y, cv=cv_splitter))\n",
        "y_pred_probs_best = best_rf.predict_proba(X_test)[:, 1]\n",
        "brier_score_best = brier_score_loss(y_test, y_pred_probs_best)\n",
        "roc_auc_best = roc_auc_score(y_test, y_pred_probs_best)\n",
        "\n",
        "# Bootstrap scores\n",
        "rf_bootstrap_scores = []\n",
        "n_bootstrap = 100\n",
        "for seed in range(8, 8 + n_bootstrap):\n",
        "    np.random.seed(seed)\n",
        "    X_test_sample, y_test_sample = resample(X_test, y_test)\n",
        "    y_pred_sample = best_rf.predict(X_test_sample)\n",
        "    score = accuracy_score(y_test_sample, y_pred_sample)\n",
        "    rf_bootstrap_scores.append(score)\n",
        "rf_average_score = np.mean(rf_bootstrap_scores)\n",
        "rf_variance_score = np.var(rf_bootstrap_scores)\n",
        "\n",
        "# Placebo test\n",
        "np.random.seed(random_seed)\n",
        "X_train_random = np.random.random(size=X_train.shape)\n",
        "rf_random = LogisticRegression()\n",
        "rf_random.fit(X_train_random, y_train)\n",
        "y_pred_random = rf_random.predict(X_test)\n",
        "accuracy_random = accuracy_score(y_test, y_pred_random)\n",
        "\n",
        "logres.append({\n",
        "    'Model': 'rf',\n",
        "    'Accuracy': accuracy_best,\n",
        "    'Cross Validation test Accuracy': cross_val_best_rf_test,\n",
        "    'Bootstrap Average Score': rf_average_score,\n",
        "    'Placebo Accuracy': accuracy_random,\n",
        "    'Cross Validation train Accuracy': cross_val_best_rf,\n",
        "    'Precision': precision_best,\n",
        "    'Recall': recall_best,\n",
        "    'F1 Score': f1_best,\n",
        "    'Brier Score': brier_score_best,\n",
        "    'ROC AUC Score': roc_auc_best,\n",
        "    'Bootstrap Variance Score': rf_variance_score\n",
        "})\n",
        "\n",
        "# Convert logres list to DataFrame\n",
        "logres_df = pd.DataFrame(logres)\n",
        "logres_df\n"
      ],
      "metadata": {
        "id": "u2kL-9kE4tqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logres_df"
      ],
      "metadata": {
        "id": "7SsyKs5C5c1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC and AUC plots\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_probs_rf = best_rf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "y_pred_probs_logit = best_logit.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and ROC area for each model\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_probs_rf)\n",
        "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
        "\n",
        "fpr_logit, tpr_logit, _ = roc_curve(y_test, y_pred_probs_logit)\n",
        "roc_auc_logit = auc(fpr_logit, tpr_logit)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_rf, tpr_rf, color='darkgreen', lw=2, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
        "plt.plot(fpr_logit, tpr_logit, color='blue', lw=2, label=f'Logistic Regression (area = {roc_auc_logit:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC and AUC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt\n"
      ],
      "metadata": {
        "id": "ccv4kC0L8Rwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision Recall curve and graph\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "y_scores_best_logit = best_logit.predict_proba(X_test)[:, 1]\n",
        "y_scores_best_rf = best_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "precision_best_logit, recall_best_logit, thresholds_best_logit = precision_recall_curve(y_test, y_scores_best_logit)\n",
        "precision_best_rf, recall_best_rf, thresholds_best_rf = precision_recall_curve(y_test, y_scores_best_rf)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(recall_best_logit, precision_best_logit, label='Logit', color='blue')\n",
        "plt.plot(recall_best_rf, precision_best_rf, label='Random Forest', color='darkgreen')\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "09zfxa1O-pO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zv9SBYd7krkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}